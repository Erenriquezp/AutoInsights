# docker-compose.yml
# Versión final para demostración de escalabilidad (2 Nodos)

services:
  # ----------------------------------------------------------------
  # 1. CAPA DE DATOS (MongoDB)
  # ----------------------------------------------------------------
  mongodb:
    image: mongo:6.0
    container_name: autoinsights_mongo
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    networks:
      - bigdata_net

  # ----------------------------------------------------------------
  # 2. CLÚSTER DE PROCESAMIENTO (SPARK) (Master + 2 Workers)
  # Aquí está la magia de la escalabilidad.
  # Usamos 'build' para que incluya tus librerías (pymongo) definidas en el Dockerfile.
  # ----------------------------------------------------------------
  # 1. SPARK MASTER (El Jefe - Se mantiene vivo esperando órdenes)
  spark-master:
    build: 
      context: ./backend/spark
      dockerfile: Dockerfile
    container_name: spark-master
    ports:
      - "8080:8080" # Panel de Control Web
      - "7077:7077" # Puerto Interno
    volumes:
      - ./backend/data:/opt/spark/data                    # Datos
      - ./backend/spark/jobs/etl_job.py:/app/etl_job.py   # Script ETL
    networks:
      - bigdata_net
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master

  # 2. WORKER 1 (Empleado 1)
  spark-worker-1:
    build: 
      context: ./backend/spark
      dockerfile: Dockerfile
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081" # Panel Web Worker 1
    volumes:
      - ./backend/data:/opt/spark/data
    depends_on:
      - spark-master
    networks:
      - bigdata_net
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  # 3. WORKER 2 (Empleado 2)
  spark-worker-2:
    build: 
      context: ./backend/spark
      dockerfile: Dockerfile
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8082:8081" # Panel Web Worker 2 (Mapeado a puerto distinto)
    volumes:
      - ./backend/data:/opt/spark/data
    depends_on:
      - spark-master
    networks:
      - bigdata_net
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  # ----------------------------------------------------------------
  # 3. CAPA DE SERVICIO (Backend API)
  # ----------------------------------------------------------------
  backend:
    build: ./backend
    container_name: autoinsights_api
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app 
    environment:
      - MONGO_URI=mongodb://mongodb:27017
    depends_on:
      - mongodb
    networks:
      - bigdata_net
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  # ----------------------------------------------------------------
  # 4. CAPA DE PRESENTACIÓN (Frontend)
  # ----------------------------------------------------------------
  frontend:
    build: ./frontend
    container_name: autoinsights_ui
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - CHOKIDAR_USEPOLLING=true
    depends_on:
      - backend
    networks:
      - bigdata_net
    stdin_open: true

volumes:
  mongo_data:

networks:
  bigdata_net:
    driver: bridge